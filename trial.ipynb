{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"close\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "#URL = \"https://www.myg.in/?match=all&subcats=Y&pcode_from_q=Y&pshort=Y&pfull=Y&pname=Y&pkeywords=Y&search_performed=Y&q=phone+&dispatch=products.search&security_hash=0a4817ffa50215e80fea73a431c5e4e2\"\n",
    "URL = \"https://www.amazon.in/hz/mobile/mission?p=Awvb5PYPCdXItKNk4syR9w%2F8ZaEI%2FEWhSLq8neDBm2ZDN%2FiUpGTmmeLEaXhYjTydaUP1RHT%2BGq3wDqgl1VsLnYaGIm60jpyMp%2FENkU67HV9BrEL%2FMuJsyHoJAVbyza4fZgpw53GCiKu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_parse(URL:str,headers:dict):\n",
    "    \"\"\"Validate the URL, fetch the webpage, and parse the content using BeautifulSoup.\"\"\"\n",
    "\n",
    "    parsed_url = urlparse(URL)\n",
    "    if not parsed_url.scheme:\n",
    "        URL = f\"http://{URL}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(URL, headers=headers)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        print(soup)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from utils import *\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:No paragraph tags found in the HTML file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No content found.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_text_from_html_file(file_path: str) -> str:\n",
    "    \"\"\"Extract text from paragraph (<p>) tags in an HTML file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text extracted from the HTML file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        for unwanted in soup([\"script\", \"style\"]):\n",
    "            unwanted.extract()\n",
    "\n",
    "\n",
    "        paragraphs = soup.find_all('p')\n",
    "        if paragraphs:\n",
    "            text_content = ' '.join(para.get_text(separator=' ', strip=True) for para in paragraphs)\n",
    "            logger.info(\"Text successfully extracted from paragraph tags.\")\n",
    "            return text_content\n",
    "        else:\n",
    "            logger.warning(\"No paragraph tags found in the HTML file.\")\n",
    "            return \"No content found.\"\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        return \"File not found.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while processing the file: {e}\")\n",
    "        return \"An error occurred.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"htmls.html\"  \n",
    "    extracted_text = extract_text_from_html_file(file_path)\n",
    "    print(extracted_text)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Asus\\\\vs_code\\\\Internship\\\\Rough'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully fetched and parsed: https://www.amazon.in/Samsung-Galaxy-Snapdragon-Phantom-Storage/dp/B0BTYX74HZ/ref=pd_ci_mcx_mh_mcx_views_0?pd_rd_w=hfy8A&content-id=amzn1.sym.fa0aca50-60f7-47ca-a90e-c40e2f4b46a9%3Aamzn1.symc.ca948091-a64d-450e-86d7-c161ca33337b&pf_rd_p=fa0aca50-60f7-47ca-a90e-c40e2f4b46a9&pf_rd_r=MHZ5Y02EQCB7YRH7TN1Q&pd_rd_wg=ZBuwO&pd_rd_r=d48e3b16-3e62-408a-b42a-17490e99d283&pd_rd_i=B0BTYX74HZ\n",
      "INFO:__main__:Text successfully extracted from paragraph tags.\n",
      "INFO:__main__:Extracted text saved to output_context.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WebPageScraper:\n",
    "    def __init__(self, url: str, headers: dict, save_path: str = \"output_context.txt\"):\n",
    "        \"\"\"\n",
    "        Initialize the WebPageScraper with the given URL, headers, and output file path.\n",
    "\n",
    "        Args:\n",
    "            url (str)       : The URL of the webpage to scrape.\n",
    "            headers (dict)  : The headers to include in the HTTP request.\n",
    "            save_path (str) : The path to the output file where extracted text will be saved.\n",
    "        \"\"\"\n",
    "        self.URL        = url\n",
    "        self.soup       = None\n",
    "        self.headers    = headers \n",
    "        self.save_path  = save_path\n",
    "\n",
    "    def fetch_and_parse(self):\n",
    "        \"\"\"Validate the URL, fetch the webpage, and parse the content using BeautifulSoup.\"\"\"\n",
    "    \n",
    "        parsed_url = urlparse(self.URL)\n",
    "        if not parsed_url.scheme:\n",
    "            self.URL = f\"http://{self.URL}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.URL, headers=self.headers)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            self.soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            logger.info(f\"Successfully fetched and parsed: {self.URL}\")\n",
    "\n",
    "        except requests.exceptions.MissingSchema:\n",
    "            logger.error(f\"Invalid URL: {self.URL} - Missing schema.\")\n",
    "            raise ValueError(f\"Invalid URL: {self.URL} - Please provide a valid URL.\")\n",
    "        \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.error(f\"Connection error while trying to reach {self.URL}.\")\n",
    "            raise ConnectionError(f\"Failed to connect to {self.URL}.\")\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(f\"Timeout error while trying to reach {self.URL}.\")\n",
    "            raise TimeoutError(f\"Request to {self.URL} timed out.\")\n",
    "        \n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            logger.error(f\"HTTP error occurred: {err}\")\n",
    "            raise RuntimeError(f\"HTTP error occurred: {err}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred: {e}\")\n",
    "            raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    def clean_and_extract_text(self) -> str:\n",
    "        \"\"\"Remove unwanted tags and extract text from paragraph (<p>) tags.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text extracted from the webpage.\n",
    "        \"\"\"\n",
    "        if self.soup:\n",
    "            for unwanted in self.soup([\"script\", \"style\", \"a\"]):\n",
    "                unwanted.extract()\n",
    "            paragraphs = self.soup.find_all('p')\n",
    "            if paragraphs:\n",
    "                text_content = ' '.join(para.get_text(separator=' ', strip=True) for para in paragraphs)\n",
    "                logger.info(\"Text successfully extracted from paragraph tags.\")\n",
    "                return text_content\n",
    "            else:\n",
    "                logger.warning(\"No paragraph tags found in the response.\")\n",
    "                return \"No content found.\"\n",
    "        logger.warning(\"No soup object available for text extraction.\")\n",
    "        return None\n",
    "\n",
    "    def save_text_to_file(self, text: str):\n",
    "        \"\"\"Save the extracted text to a specified output file.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text content to save.\n",
    "        \"\"\"\n",
    "        with open(self.save_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "            logger.info(f\"Extracted text saved to {self.save_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"close\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    #URL = \"https://www.myg.in/?match=all&subcats=Y&pcode_from_q=Y&pshort=Y&pfull=Y&pname=Y&pkeywords=Y&search_performed=Y&q=phone+&dispatch=products.search&security_hash=0a4817ffa50215e80fea73a431c5e4e2\"\n",
    "    URL = \"https://www.amazon.in/hz/mobile/mission?p=Awvb5PYPCdXItKNk4syR9w%2F8ZaEI%2FEWhSLq8neDBm2ZDN%2FiUpGTmmeLEaXhYjTydaUP1RHT%2BGq3wDqgl1VsLnYaGIm60jpyMp%2FENkU67HV9BrEL%2FMuJsyHoJAVbyza4fZgpw53GCiKuku%2Fy6GnDuUrki5PD6ijLBckEJuQwlkV8YnUajF8b2lBzL6SGo3urB4TT0o%2FR62IB%2BV2LZ0IeGB44TjQMhlNRmqGLs%2F2BBqbtKgkq5wujUsFBrDZ0FiTFqh%2Bby52Ea62F3hAMzSd%2BOTPSnmrUyiMI74568Ei%2Bew%2Bss36R%2FWtp6EjUVrIQjRTr7ofP3t9V4LiQBhk%2FtgSCTT7q71PLF%2F2t8%2FvM%2Fbi%2B%2FQp3Iaq7f5dfeJuXVim8CRIFFRTbNoNTKrlb1XZ%2FWlRnIDe5gYKbo0TfEJ1B%2F57iLhT8%3D&ref_=ci_mcx_mi&pf_rd_r=5NDCMJ3514XZZF96VSK8&pf_rd_p=652a835d-9e23-4efd-9931-74188247a57a&pd_rd_r=744eadd7-6515-444f-bbec-acd0af863342&pd_rd_w=E6X0m&pd_rd_wg=SBqVS\"\n",
    "    URL = \"https://www.amazon.in/Samsung-Galaxy-Snapdragon-Phantom-Storage/dp/B0BTYX74HZ/ref=pd_ci_mcx_mh_mcx_views_0?pd_rd_w=hfy8A&content-id=amzn1.sym.fa0aca50-60f7-47ca-a90e-c40e2f4b46a9%3Aamzn1.symc.ca948091-a64d-450e-86d7-c161ca33337b&pf_rd_p=fa0aca50-60f7-47ca-a90e-c40e2f4b46a9&pf_rd_r=MHZ5Y02EQCB7YRH7TN1Q&pd_rd_wg=ZBuwO&pd_rd_r=d48e3b16-3e62-408a-b42a-17490e99d283&pd_rd_i=B0BTYX74HZ\"    \n",
    "    scraper = WebPageScraper(url=URL, headers=headers)\n",
    "    \n",
    "    try:\n",
    "        scraper.fetch_and_parse()\n",
    "        extracted_text = scraper.clean_and_extract_text()\n",
    "        if extracted_text:\n",
    "            scraper.save_text_to_file(extracted_text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Top Brand Samsung 89% positive ratings from 100K+ customers 100K+ recent orders from this brand 11+ years on Amazon Give yourself a smartphone that recognises your emotions and responds appropriately. The Samsung Galaxy S23 5G's enhanced AI and Nightography feature produces low-light photos and videos that are vivid and colourful from dusk to dawn and back again. The Snapdragon processor in this phone also offers quick video streaming and gaming. Additionally, adaptive 120 Hz makes scrolling fluid, and Eye Comfort Shield guards against eye fatigue even while looking in low light. Samsung Galaxy S23 5G Snapdragon (Phantom Black, 8GB, 128GB Storage) Share: Found a lower price? Let us know. Fields with an asterisk * are required\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
